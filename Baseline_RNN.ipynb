{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Thesis_PreProcessing import trials_note as tn\n",
    "from Thesis_PreProcessing import ico_codes as ic\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build training graph\n",
    "\n",
    "def Baseline_RNN_graph(state_size = 200, features = 943, batch_size = 30,\n",
    "    max_steps = 40, learning_rate = 1e-3, test=False, dropout = True):\n",
    "    \n",
    "    reset_g()\n",
    "    \n",
    "    inputs = tf.placeholder(tf.float32, [None, max_steps, features], name=\"inputs\")\n",
    "    targets = tf.placeholder(tf.float32, [None, max_steps, features], name=\"targets\")\n",
    "    \n",
    "    num_layers = 2\n",
    "    keep_prob = .7\n",
    "    with tf.name_scope(\"Projection_Layer\") as scope: \n",
    "        lstm_inputs = tf.contrib.layers.fully_connected(inputs, state_size)\n",
    "    if dropout:\n",
    "        lstm_inputs = tf.map_fn(lambda x: tf.nn.dropout(x, keep_prob), lstm_inputs)\n",
    "    lstm_cell = tf.nn.rnn_cell.LSTMCell(state_size, state_is_tuple=True)\n",
    "  \n",
    "    with tf.name_scope(\"LSTM\") as scope:                                                         \n",
    "        init_state= lstm_cell.zero_state(batch_size, tf.float32)        \n",
    "        lstm_outputs, fin_states = tf.nn.dynamic_rnn(cell=lstm_cell,initial_state=init_state,  \n",
    "                                                dtype=tf.float32, sequence_length=length(inputs), inputs=lstm_inputs)\n",
    "            \n",
    "    with tf.variable_scope('sigmoid'):\n",
    "\n",
    "        W = tf.get_variable('W', [state_size, features])\n",
    "        b = tf.get_variable('b', [features], initializer=tf.constant_initializer(0.0))\n",
    "    \n",
    "   \n",
    "\n",
    "  \n",
    "    with tf.name_scope(\"Sigmoid\") as scope: \n",
    "        lstm_outputs = tf.reshape(lstm_outputs, [-1, state_size])\n",
    "        if dropout:\n",
    "            lstm_outputs = tf.map_fn(lambda x: tf.nn.dropout(x, keep_prob), lstm_outputs)\n",
    "        new_y = tf.reshape(targets, [-1, features])\n",
    "        scores = tf.matmul(lstm_outputs, W) + b\n",
    "        predictions = tf.nn.sigmoid(scores)\n",
    "        total_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=scores, labels=new_y))\n",
    "    if test==False:\n",
    "        with tf.name_scope(\"Training\") as scope:\n",
    "            train_step = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)\n",
    "    else:\n",
    "        train_step = []\n",
    "    \n",
    "    print(scores.get_shape(), new_y.get_shape)\n",
    "    return dict(\n",
    "        inputs = inputs,\n",
    "        targets = targets,\n",
    "        total_loss = total_loss,\n",
    "        train_step = train_step,\n",
    "        initial = init_state,\n",
    "        final = fin_states,\n",
    "        predictions = predictions,\n",
    "        saver = tf.train.Saver()\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data-Processing\n",
    "def length(inputs):\n",
    "    used = tf.sign(tf.reduce_max(tf.abs(inputs), reduction_indices=2))\n",
    "    length = tf.reduce_sum(used, reduction_indices=1)\n",
    "    length = tf.cast(length, tf.int32)\n",
    "    return length\n",
    "\n",
    "def dict_to_paddedXY(ico_dict, keys, max_length = 40, feat_nums=943):\n",
    "    \n",
    "    dim = len(keys)\n",
    "    X = np.zeros((dim, max_length, feat_nums))\n",
    "    Y = np.zeros((dim, max_length, feat_nums))\n",
    "    i = 0\n",
    "    for key in ico_dict:\n",
    "        if key in keys:\n",
    "            j = 0\n",
    "            dict_2 = ico_dict[key]\n",
    "            keys2 = list(dict_2.keys())\n",
    "            for key2 in dict_2:\n",
    "                X[i, j, :] = dict_2[key2]\n",
    "                if j == (len(keys2) - 1):\n",
    "                    Y[i, j, -1] = 1\n",
    "                else:\n",
    "                    Y[i, j, :] = dict_2[keys2[j+1]]\n",
    "                j += 1   \n",
    "            i += 1\n",
    "        else:\n",
    "            continue \n",
    "            \n",
    "    return (X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training/Session\n",
    "def reset_g():\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "def train_baseline(g, inputs, targets, epoch_nums=2, batch_size=30, verbose=True, save=False):\n",
    "    tf.set_random_seed(1234)\n",
    "    train_size = inputs.shape[0]\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        training_losses = []\n",
    "        \n",
    "        for i in range(epoch_nums):\n",
    "            epoch_training_loss = 0\n",
    "            idx = np.random.choice(train_size, batch_size, replace=False)\n",
    "            training_loss = 0\n",
    "            training_state = None\n",
    "            num_iters = int(train_size/batch_size)\n",
    "            for j in range(num_iters):\n",
    "                \n",
    "                X = inputs[j*batch_size:(j+1)*batch_size]\n",
    "                Y = targets[j*batch_size:(j+1)*batch_size]\n",
    "                feed_dict={g['inputs']: X, g['targets']: Y}\n",
    "                \n",
    "                if training_state is not None:\n",
    "                    training = training_state\n",
    "                    feed_dict[g['initial']] = training\n",
    "        \n",
    "                    \n",
    "                    \n",
    "                cur_training_loss, training_state, _ = sess.run([g['total_loss'],\n",
    "                                                      g['final'],\n",
    "                                                      g['train_step']],\n",
    "                                                             feed_dict)\n",
    "                training_loss += cur_training_loss\n",
    "                \n",
    "            if verbose:\n",
    "                print(\"Average training loss for Epoch\", idx, \":\", training_loss/num_iters)\n",
    "            training_losses.append(training_loss/num_iters)\n",
    "\n",
    "        if isinstance(save, str):\n",
    "            g['saver'].save(sess, save)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference/Testing\n",
    "def test_baseline(g, checkpoint, X_test, Y_test, N=20):\n",
    "    \n",
    "     with tf.Session() as sess:\n",
    "        \n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        g['saver'].restore(sess, checkpoint)\n",
    "        \n",
    "        num_iters = X_test.shape[0]\n",
    "        state = None\n",
    "        total_test_loss = 0\n",
    "        recall_accs_20 = []\n",
    "        recall_accs_25 = []\n",
    "        recall_accs_30 = []\n",
    "        avg_loss = []\n",
    "        for i in range(num_iters):\n",
    "            X = X_test[i:i+1,:,:]\n",
    "            targets = Y_test[i:i+1, :, :]\n",
    "            if state is not None:\n",
    "                state_fw, state_bw = state\n",
    "                feed_dict={g['inputs']: X, g['targets']: targets, g['initial']: state}\n",
    "            else:\n",
    "                feed_dict={g['inputs']: X, g['targets']: targets}\n",
    "            cur_test_loss, state, predictions, _ = sess.run([g['total_loss'],\n",
    "                                                      g['final'],\n",
    "                                                      g['predictions'],\n",
    "                                                      g['train_step']],\n",
    "                                                             feed_dict)\n",
    "            recall_acc_20 = recall_at_n(predictions, targets, n=20)\n",
    "            recall_acc_25 = recall_at_n(predictions, targets, n=25)\n",
    "            recall_acc_30 = recall_at_n(predictions, targets, n=30)\n",
    "            recall_accs_20.append(recall_acc_20)\n",
    "            recall_accs_25.append(recall_acc_25)\n",
    "            recall_accs_30.append(recall_acc_30)\n",
    "            total_test_loss += cur_test_loss\n",
    "            avg_loss.append(total_test_loss/float(i+1))\n",
    "            \n",
    "        avg_test_loss = total_test_loss/num_iters\n",
    "        \n",
    "        recall_20 = np.nanmean(recall_accs_20)\n",
    "        recall_25 = np.nanmean(recall_accs_25)\n",
    "        recall_30 = np.nanmean(recall_accs_30)\n",
    "        \n",
    "        writer = tf.summary.FileWriter(\"tmp/model_7\")\n",
    "        writer.flush()\n",
    "        writer.add_graph(sess.graph)\n",
    "        writer.close()\n",
    "        \n",
    "        return [avg_loss, avg_test_loss, recall_20, recall_25, recall_30]\n",
    "     \n",
    "\n",
    "\n",
    "    \n",
    "def recall_at_n(predictions, targets, n=20):\n",
    "    dim = targets.shape[1]\n",
    "    # print(dim)\n",
    "    pred_dim = 0\n",
    "    eps = .000000001\n",
    "    for i in range(dim):\n",
    "        if targets[0, i, -1] == 1:\n",
    "            pred_dim = i\n",
    "            break\n",
    "    if pred_dim != 0:\n",
    "        valid_targets = targets[0, 0:pred_dim, :]\n",
    "        valid_preds = predictions[0:pred_dim, :]\n",
    "        recall_val = np.zeros((pred_dim,))\n",
    "        for i in range(pred_dim):\n",
    "\n",
    "            pos_count = 0.0\n",
    "            pred_pos = 0.0\n",
    "            \n",
    "            for j in range(n):\n",
    "              \n",
    "                if np.sort(-valid_targets)[i][j] <= (-1. + eps) and np.sort(-valid_targets)[i][j] >= (-1. - eps):\n",
    "                    pos_count += 1.\n",
    "\n",
    "            for j in range(n):\n",
    "                k = int(pos_count)\n",
    "                if np.argsort(-valid_preds)[i][j] in np.argsort(-valid_targets)[i][0:k]:\n",
    "                    pred_pos += 1.\n",
    "                    \n",
    "\n",
    "            if pos_count != 0.0:\n",
    "                recall_val[i] = pred_pos/float(pos_count)\n",
    "            else:\n",
    "                recall_val[i] = 0.0\n",
    "\n",
    "        return np.mean(recall_val)\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(X_train, Y_train, X_val, Y_val, s_size = [200, 300, 400], b_size = [20, 30, 40], learn_rate = 1e-3):\n",
    "    for ss in s_size:\n",
    "        for bs in b_size:\n",
    "            print(\"State size: \" + str(ss) + \" Batch_Size: \" + str(bs))\n",
    "            g = Baseline_RNN_graph(state_size=ss, batch_size = bs, learning_rate=learn_rate)\n",
    "            train_baseline(g, X_train, Y_train, batch_size=bs, save=\"/tmp/val.ckpt\")\n",
    "            g_2 = Baseline_RNN_graph(state_size=ss, learning_rate=learn_rate, batch_size=1, test=True)\n",
    "            losses, loss, recall_20, recall_25, recall_30 = test_baseline(g_2, \"/tmp/val.ckpt\", X_val, Y_val)\n",
    "            print(loss)\n",
    "            print(recall_20, recall_25, recall_30)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main/Run\n",
    "current_connection = tn.connect()\n",
    "current_cur = current_connection[1]\n",
    "\n",
    "ico_dict = ic.ico_dict_populator(current_cur)\n",
    "ico_dict_keys = list(ico_dict.keys())\n",
    "ico_dict_keys = np.array(ico_dict_keys)\n",
    "tr_idx = int(.7*len(ico_dict_keys))\n",
    "val_idx = int(.85*len(ico_dict_keys))\n",
    "train_keys = ico_dict_keys[:tr_idx]\n",
    "val_keys = ico_dict_keys[tr_idx+1: val_idx]\n",
    "test_keys = ico_dict_keys[val_idx:]\n",
    "\n",
    "print(train_keys)\n",
    "\n",
    "X_train, Y_train = dict_to_paddedXY(ico_dict, train_keys)\n",
    "X_val, Y_val = dict_to_paddedXY(ico_dict, val_keys)\n",
    "X_test, Y_test = dict_to_paddedXY(ico_dict, test_keys)\n",
    "g_1 = Baseline_RNN_graph()\n",
    "train_baseline(g_1, X_train, Y_train, epoch_nums=3, save=\"/tmp/model_6.ckpt\")\n",
    "g_2 = Baseline_RNN_graph(batch_size = 1, test=True, dropout=False)\n",
    "losses, loss, recall_20, recall_25, recall_30 = test_baseline(g_2, \"/tmp/model_6.ckpt\", X_test, Y_test)\n",
    "\n",
    "# cross_validation(X_train, Y_train, X_val, Y_val)\n",
    "tn.close(current_connection[0], current_connection[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p27)",
   "language": "python",
   "name": "conda_tensorflow_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
