{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Thesis_PreProcessing import trials_note as tn\n",
    "from Thesis_PreProcessing import encoder_train_ico as eti\n",
    "from Thesis_PreProcessing import ico_codes as ic\n",
    "from Thesis_PreProcessing import more_io as mi\n",
    "import collections\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "\n",
    "\n",
    "import hashlib\n",
    "import numbers\n",
    "\n",
    "from tensorflow.python.eager import context\n",
    "from tensorflow.python.framework import constant_op\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import tensor_shape\n",
    "from tensorflow.python.framework import tensor_util\n",
    "from tensorflow.python.layers import base as base_layer\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import clip_ops\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import nn_ops\n",
    "from tensorflow.python.ops import partitioned_variables\n",
    "from tensorflow.python.ops import random_ops\n",
    "from tensorflow.python.ops import tensor_array_ops\n",
    "from tensorflow.python.ops import variable_scope as vs\n",
    "from tensorflow.python.ops import variables as tf_variables\n",
    "from tensorflow.python.platform import tf_logging as logging\n",
    "from tensorflow.python.util import nest\n",
    "from tensorflow.python.util.tf_export import tf_export\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From TensorFlow package\n",
    "class LayerRNNCell(tf.nn.rnn_cell.RNNCell):\n",
    "  \"\"\"Subclass of RNNCells that act like proper `tf.Layer` objects.\n",
    "  For backwards compatibility purposes, most `RNNCell` instances allow their\n",
    "  `call` methods to instantiate variables via `tf.get_variable`.  The underlying\n",
    "  variable scope thus keeps track of any variables, and returning cached\n",
    "  versions.  This is atypical of `tf.layer` objects, which separate this\n",
    "  part of layer building into a `build` method that is only called once.\n",
    "  Here we provide a subclass for `RNNCell` objects that act exactly as\n",
    "  `Layer` objects do.  They must provide a `build` method and their\n",
    "  `call` methods do not access Variables `tf.get_variable`.\n",
    "  \"\"\"\n",
    "\n",
    "  def __call__(self, inputs, state, scope=None, *args, **kwargs):\n",
    "    \"\"\"Run this RNN cell on inputs, starting from the given state.\n",
    "    Args:\n",
    "      inputs: `2-D` tensor with shape `[batch_size, input_size]`.\n",
    "      state: if `self.state_size` is an integer, this should be a `2-D Tensor`\n",
    "        with shape `[batch_size, self.state_size]`.  Otherwise, if\n",
    "        `self.state_size` is a tuple of integers, this should be a tuple\n",
    "        with shapes `[batch_size, s] for s in self.state_size`.\n",
    "      scope: optional cell scope.\n",
    "      *args: Additional positional arguments.\n",
    "      **kwargs: Additional keyword arguments.\n",
    "    Returns:\n",
    "      A pair containing:\n",
    "      - Output: A `2-D` tensor with shape `[batch_size, self.output_size]`.\n",
    "      - New state: Either a single `2-D` tensor, or a tuple of tensors matching\n",
    "        the arity and shapes of `state`.\n",
    "    \"\"\"\n",
    "    # Bypass RNNCell's variable capturing semantics for LayerRNNCell.\n",
    "    # Instead, it is up to subclasses to provide a proper build\n",
    "    # method.  See the class docstring for more details.\n",
    "    return base_layer.Layer.__call__(self, inputs, state, scope=scope,\n",
    "                                     *args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build RRA cell\n",
    "\n",
    "_WEIGHTS_VARIABLE_NAME = \"W_r\"\n",
    "_BIAS_VARIABLE_NAME = \"b\"\n",
    "_ATTENTION_VARIABLE_NAME = \"W_a\"\n",
    "\n",
    "#_RRAStateTuple = collections.namedtuple(\"RRAStateTuple\", (\"c\", \"h_1\"))\n",
    "_RRAStateTuple = collections.namedtuple(\"RRAStateTuple\", (\"c\", \"h_1\", \"h_2\", \"h_3\", \"h_4\", \"h_5\"))\n",
    "\n",
    "# Inspired by TensorFlow implementation\n",
    "class RRAStateTuple(_RRAStateTuple):\n",
    " \n",
    "\n",
    "  __slots__ = ()\n",
    "\n",
    "  @property\n",
    "  def dtype(self):\n",
    "    (c, h_1, h_2, h_3, h_4, h_5) = self\n",
    "    # (c, h_1) = self\n",
    "    if c.dtype != h_1.dtype:\n",
    "      raise TypeError(\"Inconsistent internal state: %s vs %s\" %\n",
    "                      (str(c.dtype), str(h_1.dtype)))\n",
    "    return c.dtype\n",
    "\n",
    "\n",
    "class RRA_Cell(LayerRNNCell):\n",
    "    \n",
    "    def __init__(self, num_units, batchsize, forget_bias=1.0,\n",
    "                activation=None, reuse=None, name=None):\n",
    "  \n",
    "    \n",
    "        super(RRA_Cell, self).__init__(_reuse=reuse, name=name)\n",
    "    \n",
    "        # Inputs must be 2-dimensional.\n",
    "        self.input_spec = base_layer.InputSpec(ndim=2)\n",
    "        self._num_units = num_units\n",
    "        self._forget_bias = forget_bias\n",
    "        self._activation = activation or math_ops.tanh\n",
    "        self.batch_size = batchsize\n",
    "    @property    \n",
    "    def output_size(self):\n",
    "        return self._num_units\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        #print()\n",
    "        return RRAStateTuple(self._num_units, self._num_units, self._num_units, self._num_units, self._num_units, \n",
    "                            self._num_units)\n",
    "    \n",
    "    def build(self, inputs_shape):\n",
    "        if inputs_shape[1].value is None:\n",
    "              raise ValueError(\"Expected inputs.shape[-1] to be known, saw shape: %s\"\n",
    "                       % inputs_shape)\n",
    "\n",
    "        input_depth = inputs_shape[1].value\n",
    "        h_depth = self._num_units\n",
    "        self._kernel = self.add_variable(\n",
    "            _WEIGHTS_VARIABLE_NAME,\n",
    "            shape=[input_depth + h_depth, 4 * self._num_units])\n",
    "        self._attention_kernel = self.add_variable(\n",
    "            _ATTENTION_VARIABLE_NAME,\n",
    "            shape=[1 , 4])\n",
    "        self._bias = self.add_variable(\n",
    "            _BIAS_VARIABLE_NAME,\n",
    "            shape=[4* self._num_units],\n",
    "            initializer=init_ops.zeros_initializer(dtype=self.dtype))\n",
    "\n",
    "        self.built = True\n",
    "        \n",
    "    def call(self, inputs, state):\n",
    "        \n",
    "        sigmoid = math_ops.sigmoid\n",
    "        one = constant_op.constant(1, dtype=dtypes.int32)\n",
    "        c, h_1, h_2, h_3, h_4, h_5 = state\n",
    "        \n",
    "        gate_inputs = math_ops.matmul(array_ops.concat([inputs, h_1], 1), self._kernel)\n",
    "        gate_inputs = nn_ops.bias_add(gate_inputs, self._bias)\n",
    "        \n",
    "        i, j, f, o = array_ops.split(value=gate_inputs, num_or_size_splits=4, axis=one)\n",
    "        forget_bias_tensor = constant_op.constant(self._forget_bias, dtype=f.dtype)\n",
    "        \n",
    "        add = math_ops.add\n",
    "        multiply = math_ops.multiply\n",
    "        divide = math_ops.div\n",
    "        \n",
    "        new_c = add(multiply(c, sigmoid(add(f, forget_bias_tensor))),\n",
    "                multiply(sigmoid(i), self._activation(j)))\n",
    "        \n",
    "        attention_weights = divide(self._attention_kernel, tf.reduce_sum(self._attention_kernel, 1))\n",
    "        h_stack = array_ops.stack([h_2, h_3, h_4, h_5], 0)\n",
    "        h_stack = tf.reshape(h_stack, [4, -1])\n",
    "        a = tf.reshape(math_ops.matmul(attention_weights, h_stack), [self.batch_size, 300])\n",
    "        \n",
    "        new_h = multiply(o, self._activation(add(new_c, a)))\n",
    "        new_state = RRAStateTuple(new_c, new_h, h_1, h_2, h_3, h_4)\n",
    "\n",
    "        return new_h, new_state \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build training graph\n",
    "\n",
    "def Encoder_RNN_graph(feature_size, class_size = 943, state_size = 300, batch_size = 30,\n",
    "    max_steps = 100, learning_rate = 1e-3, test=False, LSTM=False):\n",
    "    \n",
    "    reset_g()\n",
    "    \n",
    "    inputs = tf.placeholder(tf.float32, [None, max_steps, feature_size], name=\"input_sequence_ph\")\n",
    "    targets = tf.placeholder(tf.float32, [None, class_size], name=\"target_sequence_ph\")\n",
    "    lstm_inputs = tf.contrib.layers.fully_connected(inputs, state_size)\n",
    "    if not LSTM:\n",
    "        lstm_cell = RRA_Cell(state_size, batch_size)\n",
    "    else:\n",
    "        lstm_cell = tf.nn.rnn_cell.LSTMCell(state_size, state_is_tuple=True)\n",
    "    init_state = lstm_cell.zero_state(batch_size, tf.float32)\n",
    "    lstm_outputs, fin_states = tf.nn.dynamic_rnn(cell=lstm_cell, initial_state=init_state, dtype=tf.float32, \n",
    "                                                               sequence_length=length(inputs), inputs=lstm_inputs)\n",
    "\n",
    "    if not LSTM:\n",
    "        c, final_hidden, h_2, h_3, h_4, h_5 = fin_states\n",
    "    else:\n",
    "        c, final_hidden = fin_states\n",
    "   \n",
    "    \n",
    "    with tf.variable_scope('sigmoid'):\n",
    "        W = tf.get_variable('W', [state_size, class_size])\n",
    "        b = tf.get_variable('b', [class_size], initializer=tf.constant_initializer(0.0))\n",
    "    \n",
    "\n",
    "    scores = tf.matmul(final_hidden, W) + b\n",
    "    predictions = tf.nn.sigmoid(scores)\n",
    "    \n",
    "    total_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=scores, labels=targets))\n",
    "    if not test:\n",
    "        train_step = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)\n",
    "    else:\n",
    "        train_step = []\n",
    "    \n",
    "    # print(scores.get_shape(), targets.get_shape)\n",
    "    return dict(\n",
    "        inputs = inputs,\n",
    "        targets = targets,\n",
    "#        relevant = relevant_outputs,\n",
    "        total_loss = total_loss,\n",
    "        train_step = train_step,\n",
    "        initial = init_state,\n",
    "        final = fin_states,\n",
    "        predictions = predictions,\n",
    "        saver = tf.train.Saver()\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_relevant(output, length):\n",
    "    batch_size = tf.shape(output)[0]\n",
    "    max_length = tf.shape(output)[1]\n",
    "    out_size = int(output.get_shape()[2])\n",
    "    index = tf.range(0, batch_size) * max_length + (length - 1)\n",
    "    flat = tf.reshape(output, [-1, out_size])\n",
    "    relevant = tf.gather(flat, index)\n",
    "    return relevant\n",
    "\n",
    "def length(inputs):\n",
    "    used = tf.sign(tf.reduce_max(tf.abs(inputs), reduction_indices=2))\n",
    "    length = tf.reduce_sum(used, reduction_indices=1)\n",
    "    length = tf.cast(length, tf.int32)\n",
    "    return length\n",
    "\n",
    "def encoder_dict_to_paddedXY(note_dict, ico_dict, keys, test=False, max_length = 100, \n",
    "                             note_feats=3145 ,ico_feats=943):\n",
    "    dim = len(keys)\n",
    "    if test:\n",
    "        print(\"TEST!\")\n",
    "        dim = 0\n",
    "        for key in note_dict:\n",
    "            if key in keys:\n",
    "                note_dict[key] = visit_dict\n",
    "                for visit in visit_dict:\n",
    "                    dim += 1\n",
    "        print dim \n",
    "    X = np.zeros((dim, max_length, note_feats), dtype=(np.float16))\n",
    "    Y = np.zeros((dim, ico_feats), dtype=(np.float16))\n",
    "    i = 0\n",
    "    for subject in note_dict:\n",
    "     \n",
    "        if subject in keys:\n",
    "      \n",
    "            j = 0\n",
    "            note_dict_2 = note_dict[subject]\n",
    "            if subject in ico_dict:\n",
    "                ico_dict_2 = ico_dict[subject]\n",
    "               \n",
    "            else:\n",
    "                print(\"WRONG!\")\n",
    "            for visit in note_dict_2:\n",
    "              \n",
    "                mat = note_dict_2[visit]\n",
    "             \n",
    "                depth = mat.shape[0]\n",
    "                X[i, 0: depth, :] = mat\n",
    "                if visit in ico_dict_2:\n",
    "                    Y[i, :] = ico_dict_2[visit]\n",
    "                i += 1\n",
    "                \n",
    "        else:\n",
    "            continue \n",
    "            \n",
    "    return (X,Y)\n",
    "\n",
    "def encoder_dict_test_XY(visit_notes, visit_ico, visit, max_length = 100, \n",
    "        note_feats=3145 ,ico_feats=943):\n",
    "    dim = 1\n",
    "    X = np.zeros((dim, max_length, note_feats), dtype=(np.float16))\n",
    "    Y = np.zeros((dim, ico_feats), dtype=(np.float16))\n",
    "    mat = visit_notes[visit]\n",
    "    if mat.shape[0] > 100:\n",
    "        idx = list(np.random.choice(mat.shape[0], 100))\n",
    "        mat = mat[idx]\n",
    "                    \n",
    "    depth = mat.shape[0]\n",
    "    X[0, 0:depth, :] = mat\n",
    "    Y[0, :] = visit_ico[visit]\n",
    "    \n",
    "    return X,Y\n",
    "        \n",
    "    \n",
    "\n",
    "def reset_g():\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_encoder(g, inputs, targets, epoch_nums=2, batch_size=30, verbose=True, save=False):\n",
    "    tf.set_random_seed(1234)\n",
    "    train_size = inputs.shape[0]\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        training_losses = []\n",
    "        \n",
    "        for i in range(epoch_nums):\n",
    "            epoch_training_loss = 0\n",
    "            idx = np.random.choice(train_size, batch_size, replace=False)\n",
    "            training_loss = 0\n",
    "            training_state = None\n",
    "            num_iters = int(train_size/batch_size)\n",
    "            for j in range(num_iters):\n",
    "                \n",
    "                X = inputs[j*batch_size:(j+1)*batch_size]\n",
    "                Y = targets[j*batch_size:(j+1)*batch_size]\n",
    "                feed_dict={g['inputs']: X, g['targets']: Y}\n",
    "                \n",
    "                if training_state is not None:\n",
    "                    training = training_state\n",
    "                    feed_dict[g['initial']] = training\n",
    "                \n",
    "                    \n",
    "                cur_training_loss, training_state, _ = sess.run([g['total_loss'],\n",
    "                                                      g['final'],\n",
    "                                                      g['train_step']],\n",
    "                                                             feed_dict)\n",
    "               \n",
    "                training_loss += cur_training_loss\n",
    "                \n",
    "            if verbose:\n",
    "                print(\"Average training loss for Epoch\", idx, \":\", training_loss/num_iters)\n",
    "            training_losses.append(training_loss/num_iters)\n",
    "\n",
    "        if isinstance(save, str):\n",
    "            g['saver'].save(sess, save)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference/Testing\n",
    "def test_encoder(g, checkpoint, X_test, Y_test, N=20):\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        g['saver'].restore(sess, checkpoint)\n",
    "        \n",
    "        num_iters = X_test.shape[0]\n",
    "        state = None\n",
    "        total_test_loss = 0\n",
    "        recall_accs = []\n",
    "        for i in range(num_iters):\n",
    "            X = X_test[i:i+1,:,:]\n",
    "            targets = Y_test[i:i+1, :]\n",
    "    \n",
    "            if state is not None:\n",
    "                state_fw = state\n",
    "                feed_dict={g['inputs']: X, g['targets']: targets, g['initial']: state}\n",
    "            else:\n",
    "                feed_dict={g['inputs']: X, g['targets']: targets}\n",
    "            cur_test_loss, state, predictions, _ = sess.run([g['total_loss'],\n",
    "                                                      g['final'],\n",
    "                                                      g['predictions'],\n",
    "                                                      g['train_step']],\n",
    "                                                             feed_dict)\n",
    "            recall_acc = recall_at_n(predictions, targets, n=N)\n",
    "            recall_accs.append(recall_acc)\n",
    "            total_test_loss += cur_test_loss\n",
    "            \n",
    "        avg_test_loss = total_test_loss/num_iters\n",
    "        \n",
    "    return [avg_test_loss, recall_accs]\n",
    "     \n",
    "\n",
    "    \n",
    "def recall_at_n(predictions, targets, n=20):\n",
    "    dim = targets.shape[1]\n",
    "\n",
    "    eps = .000000001\n",
    " \n",
    "    valid_targets = targets[0, :]\n",
    "    valid_preds = predictions[0, :]\n",
    "\n",
    "\n",
    "\n",
    "    pos_count = 0.0\n",
    "    pred_pos = 0.0\n",
    "            \n",
    "    for j in range(n):  \n",
    "        if np.sort(-valid_targets)[j] <= (-1. + eps) and np.sort(-valid_targets)[j] >= (-1. - eps):\n",
    "            pos_count += 1.\n",
    "      \n",
    "    for j in range(n):\n",
    "        k = int(pos_count)\n",
    "        if np.argsort(-valid_preds)[j] in np.argsort(-valid_targets)[0:k]:\n",
    "                pred_pos += 1.\n",
    "                    \n",
    "\n",
    "    if pos_count != 0.0:\n",
    "        recall_val = pred_pos/float(pos_count)\n",
    "    else:\n",
    "        recall_val = 0.0\n",
    "\n",
    "    return recall_val\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_matrix(g, checkpoint, note_visits, ico_visits, LSTM):\n",
    "    visits = note_visits.keys()\n",
    "    final_state_vec = []\n",
    "    for visit in visits:\n",
    "        X, Y = encoder_dict_test_XY(note_visits, ico_visits, visit)\n",
    "        with tf.Session() as sess:\n",
    "        \n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            g['saver'].restore(sess, checkpoint)\n",
    "        \n",
    "            init = None\n",
    "            feed_dict = {g['inputs']: X, g['targets']: Y}\n",
    "            final_states = sess.run(g['final'], feed_dict)\n",
    "            if not LSTM:\n",
    "                c, final_state, h_2, h_3, h_4, h_5 = final_states\n",
    "            else:\n",
    "                c, final_state = final_states\n",
    "            final_state_vec.append(final_state)\n",
    "        \n",
    "            \n",
    "    return final_state_vec\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_representation_creator(g, checkpoint, ico_dict, note_dict, keys, LSTM=False):\n",
    "    dense_dict = {}\n",
    "    for subject in ico_dict:\n",
    "        if subject in keys:\n",
    "            visit_dict = ico_dict[subject]\n",
    "            note_visits = note_dict[subject]\n",
    "            dense_reps = dense_matrix(g, checkpoint, note_visits, visit_dict, LSTM)\n",
    "            for i, visit in enumerate(visit_dict):\n",
    "                if subject not in dense_dict:\n",
    "                    dense_dict[subject] = {visit: []}\n",
    "                else:\n",
    "                    dense_dict[subject][visit] = []\n",
    "                dense_dict[subject][visit] = dense_reps[i]\n",
    "    return dense_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_connection = tn.connect()\n",
    "current_cur = current_connection[1]\n",
    "\n",
    "# ICD9_Data_first\n",
    "train_ico_dict = eti.ico_dict_pop_encoder(current_cur)\n",
    "train_ico_dict_keys = list(train_ico_dict.keys())\n",
    "# ico_dict_keys = np.array(ico_dict_keys)\n",
    "tr_idx = int(.18*len(train_ico_dict_keys))\n",
    "val_idx = int(.23*len(train_ico_dict_keys))\n",
    "train_keys = train_ico_dict_keys[:tr_idx]\n",
    "test_keys = train_ico_dict_keys[tr_idx+1: val_idx]\n",
    "#test_keys = ico_dict_keys[val_idx:]\n",
    "\n",
    "#ICD9_Data_final\n",
    "embed_ico_dict = ic.ico_dict_populator(current_cur)\n",
    "embed_ico_dict_keys = list(embed_ico_dict.keys())\n",
    "samp = int(.18*len(embed_ico_dict_keys))\n",
    "sample_keys = embed_ico_dict_keys[:samp]\n",
    "\n",
    "\n",
    "# Note Data\n",
    "with open('Thesis_PreProcessing/cui_map_train.pickle', 'rb') as cui_map_train, open('Thesis_PreProcessing/cui_map_test.pickle', 'rb') as cui_map_test:\n",
    "    train_map = pickle.load(cui_map_train)\n",
    "    test_map = pickle.load(cui_map_test)\n",
    "with open('Thesis_PreProcessing/cui_dict_train.pickle', 'rb') as cui_dict_train, open('Thesis_PreProcessing/cui_dict_test.pickle', 'rb') as cui_dict_test:\n",
    "    train_dict = pickle.load(cui_dict_train)\n",
    "    test_dict = pickle.load(cui_dict_test)\n",
    "    \n",
    "merged_map = mi.map_merge(train_map, test_map)\n",
    "train_vectorized = mi.cui_vectorizer(train_dict, merged_map)\n",
    "test_vectorized = mi.cui_vectorizer(test_dict, merged_map)\n",
    "\n",
    "old_X_Train = {}\n",
    "old_Y_Train = {}\n",
    "\n",
    "\n",
    "X_train, Y_train = encoder_dict_to_paddedXY(train_vectorized, train_ico_dict, train_keys)\n",
    "X_test, Y_test = encoder_dict_to_paddedXY(train_vectorized, train_ico_dict, test_keys)\n",
    "\n",
    "\n",
    "g_1 = Encoder_RNN_graph(3145, LSTM=True)\n",
    "train_encoder(g_1, X_train, Y_train, epoch_nums=4, save='/tmp/encoder_LSTM_4.ckpt')\n",
    "g_2 = Encoder_RNN_graph(3145, batch_size=1, test=True, LSTM=True)\n",
    "loss, recalls = test_encoder(g_2, '/tmp/encoder_RRA_2.ckpt', X_test, Y_tt)\n",
    "dense_embedding = dense_representation_creator(g_2, '/tmp/encoder_LSTM_4.ckpt', embed_ico_dict, test_vectorized, embed_ico_dict_keys, LSTM=True)\n",
    "\n",
    "with open('Thesis_PreProcessing/dense_reps_LSTM.pickle','wb') as dense_reps:\n",
    "    pickle.dump(dense_embedding, dense_reps, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p27)",
   "language": "python",
   "name": "conda_tensorflow_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
